# Testing Philosophy: Unit, Integration, and E2E Testing in Modern Web Apps

## What We Built

We just set up a complete three-tier testing infrastructure for BroteinBuddy using Vitest for unit and integration tests, Playwright for end-to-end tests, and Testing Library for component testing. This isn't just about installing some packages - we established a comprehensive testing strategy that will guide how we verify our application works correctly throughout development.

Think of it like building a safety net with different mesh sizes. Unit tests are the fine mesh that catches small bugs in individual functions. Integration tests are the medium mesh that catches issues in how components work together. E2E tests are the coarse mesh that catches problems in complete user workflows. Together, they give us confidence that our app works correctly from the smallest utility function up to full user journeys.

We configured everything with a 90% coverage threshold, meaning at least 90% of our code must be exercised by tests. We also set up mobile-first E2E testing, because this is a PWA designed for iOS, so we test on an iPhone viewport first.

## Why This Approach

### The Problem We Solved

When building web applications, you need to verify they work correctly. But "testing" is a broad term - there are many types of tests you could write. The question is: what should you test, and how?

Without a clear testing strategy, you end up with:

- **Under-testing**: Missing bugs that users will find
- **Over-testing**: Wasting time testing implementation details that don't matter
- **Wrong testing**: Testing things at the wrong level (like E2E testing pure logic)
- **Brittle tests**: Tests that break when you refactor, even though behavior didn't change

We needed a structured approach that tells us what to test, how to test it, and when each type of test is appropriate.

### The Testing Pyramid

The industry-standard model is the **Testing Pyramid**, which says you should have:

```
        /\
       /E2E\         <- Fewest tests, slowest, highest confidence
      /------\
     /  Int.  \      <- Moderate number, moderate speed
    /----------\
   /    Unit    \    <- Most tests, fastest, lowest confidence
  /--------------\
```

This pyramid represents:

- **Base (Unit)**: Lots of fast, focused tests for individual functions
- **Middle (Integration)**: Moderate number of tests for components and their interactions
- **Top (E2E)**: Fewer tests for complete user workflows

**Why this shape?** Each level up the pyramid:

- Takes longer to run (unit = milliseconds, integration = seconds, E2E = 10+ seconds)
- Tests more of the system (unit = one function, E2E = entire app)
- Is more expensive to maintain (E2E tests break more often)
- Gives more confidence (E2E verifies real user experience)

### Options We Considered

#### Option 1: Only E2E Tests

**The "test like a user" approach** - just use Playwright for everything.

- Pros: Tests the real user experience, catches integration bugs
- Cons: Extremely slow (minutes to run full suite), brittle (break on UI changes), hard to debug (where did it break?), expensive to run in CI
- **Why we didn't choose it**: You'd wait forever for feedback, and you can't test edge cases easily

#### Option 2: Only Unit Tests

**The "pure TDD" approach** - test every function in isolation with mocks.

- Pros: Super fast, easy to debug, great coverage numbers
- Cons: Doesn't test integration, mocks can lie (mocked behavior != real behavior), doesn't test UI, can't verify user workflows
- **Why we didn't choose it**: You'd have 100% coverage but the app might not actually work when components interact

#### Option 3: Three-Tier Strategy (Unit + Integration + E2E) ← **We chose this**

**The "testing pyramid" approach** - use the right test type for each concern.

- Pros: Fast feedback (unit tests run in <1s), good coverage, tests both pieces and the whole, catches bugs at all levels
- Cons: More complex setup, need to learn multiple tools, requires discipline to test at the right level
- **Why we chose it**: Balances speed, coverage, and confidence. You get fast feedback from unit tests during development, confidence from integration tests that components work, and validation from E2E tests that user workflows succeed.

## How It Works

### The Three Test Types

#### 1. Unit Tests: Pure Logic

**What:** Test individual functions in complete isolation, no DOM, no components, no external dependencies.

**When to use:**

- Pure utility functions (capitalize, truncate, formatDate)
- Algorithm implementations (weighted random selection, box priority sorting)
- Business logic (inventory calculations, conflict detection)
- Type guards or validators

**Example from our codebase** (`tests/unit/string.test.ts`):

```typescript
import { describe, it, expect } from 'vitest';
import { capitalize, truncate } from '../../src/lib/utils/string';

describe('capitalize', () => {
  it('capitalizes the first letter of a string', () => {
    expect(capitalize('hello')).toBe('Hello');
    expect(capitalize('world')).toBe('World');
  });

  it('handles empty strings', () => {
    expect(capitalize('')).toBe('');
  });
});
```

**Why this works:** We're testing `capitalize` as a pure function. Input goes in, output comes out. No rendering, no user interaction, no DOM. This runs in ~1 millisecond.

**What we're NOT testing:** We're not testing whether this function is called correctly by components. That's integration testing's job.

#### 2. Integration Tests: Components + User Interaction

**What:** Test Svelte components with simulated user interaction, using Testing Library to interact with the DOM like a user would.

**When to use:**

- Component rendering and display
- User interactions (clicks, typing, form submission)
- Component state changes
- Conditional rendering
- Props and events
- Accessibility (can users actually use this?)

**Example from our codebase** (`tests/integration/Counter.test.ts`):

```typescript
import { describe, it, expect } from 'vitest';
import { render, screen } from '@testing-library/svelte';
import userEvent from '@testing-library/user-event';
import Counter from '../../src/lib/components/Counter.svelte';

describe('Counter Component', () => {
  it('increments count when + button is clicked', async () => {
    const user = userEvent.setup();
    render(Counter);

    const incrementButton = screen.getByTestId('increment');
    await user.click(incrementButton);

    const countElement = screen.getByTestId('count');
    expect(countElement.textContent).toBe('1');
  });
});
```

**Why this works:** We render the actual component, simulate a real click, and verify the DOM updates. This tests the component's behavior from a user's perspective. No implementation details - we don't care how the count is stored internally, just that clicking "+" makes it go up.

**Key principle:** Test behavior, not implementation. We use `getByTestId` (or better: `getByRole`, `getByLabelText`) to find elements like a user would, not by reaching into component internals.

#### 3. E2E Tests: Complete User Workflows

**What:** Test the entire application running in a real browser, from the user's perspective, with no mocking.

**When to use:**

- Critical user journeys (login → select → checkout)
- Multi-page workflows
- Real browser behavior (navigation, localStorage, network)
- Cross-browser compatibility
- Mobile viewport testing

**Example from our codebase** (`tests/e2e/home.spec.ts`):

```typescript
import { test, expect } from '@playwright/test';

test.describe('Home Page', () => {
  test('loads successfully and displays the app', async ({ page }) => {
    await page.goto('/');

    // Check that the page loaded
    await expect(page).toHaveTitle(/Vite \+ Svelte \+ TS/);

    // Verify basic page structure exists
    const body = page.locator('body');
    await expect(body).toBeVisible();
  });

  test('has proper viewport for mobile', async ({ page }) => {
    await page.goto('/');

    // Check viewport dimensions (this will use iPhone 13 Pro from config)
    const viewport = page.viewportSize();
    expect(viewport).toBeTruthy();
  });
});
```

**Why this works:** This runs the actual app in a real browser (or headless browser). It starts the dev server, navigates to the page, and verifies it loads. No mocking, no simulation - this is the real deal.

**What we're testing:** The entire stack - Vite builds the app, the browser loads it, JavaScript executes, the page renders. If this passes, we know the app at least boots up.

### Key Components of Our Setup

#### Vitest Configuration (`vitest.config.ts`)

```typescript
export default defineConfig({
  plugins: [svelte({ hot: !process.env.VITEST })],
  test: {
    globals: true,
    environment: 'jsdom',
    include: ['tests/unit/**/*.test.ts', 'tests/integration/**/*.test.ts'],
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html'],
      thresholds: {
        lines: 90,
        functions: 90,
        branches: 90,
        statements: 90,
      },
    },
  },
});
```

**What this does:**

- **`plugins: [svelte()]`**: Allows Vitest to understand `.svelte` files
- **`environment: 'jsdom'`**: Provides a fake DOM for component tests (faster than real browser)
- **`globals: true`**: Makes `describe`, `it`, `expect` available without imports (optional, but convenient)
- **`coverage.thresholds`**: Enforces 90% coverage - build fails if below this
- **`include`**: Only runs unit and integration tests with Vitest (not E2E)

#### Playwright Configuration (`playwright.config.ts`)

```typescript
export default defineConfig({
  testDir: './tests/e2e',
  use: {
    baseURL: 'http://localhost:5173',
    trace: 'on-first-retry',
  },
  projects: [
    {
      name: 'Mobile Safari',
      use: { ...devices['iPhone 13 Pro'] },
    },
    {
      name: 'Desktop Chrome',
      use: { ...devices['Desktop Chrome'] },
    },
  ],
  webServer: {
    command: 'npm run dev',
    url: 'http://localhost:5173',
    reuseExistingServer: !process.env.CI,
  },
});
```

**What this does:**

- **`testDir: './tests/e2e'`**: Playwright only runs E2E tests
- **`projects`**: Runs tests in multiple browsers/viewports (mobile first!)
- **`webServer`**: Automatically starts dev server before tests, shuts down after
- **`trace: 'on-first-retry'`**: Records traces for debugging when tests fail

**Key decision:** We test on **iPhone 13 Pro first**, then Desktop Chrome. Why? Because this is a mobile PWA. If it works on mobile, desktop is usually fine. If we only tested desktop, we might miss mobile-specific issues (touch targets too small, viewport too narrow).

#### Test Directory Structure

```
tests/
├── unit/
│   └── string.test.ts          # Pure logic tests
├── integration/
│   └── Counter.test.ts         # Component tests
└── e2e/
    └── home.spec.ts            # Full workflow tests
```

**Naming convention:**

- Vitest tests: `.test.ts`
- Playwright tests: `.spec.ts`

This is just convention, but it helps you instantly know what kind of test you're looking at.

## Design Trade-offs

### What We Optimized For

**1. Fast Feedback During Development**

Unit tests run in milliseconds. When you're developing a new algorithm (like weighted random selection), you want instant feedback. Save the file, tests run, you see red or green immediately. This tight feedback loop is critical for TDD and fast iteration.

**2. Confidence Without Slowness**

We could achieve 100% E2E coverage, but it would take 10+ minutes to run all tests. Instead, we use unit tests for most coverage (fast), integration tests for component behavior (medium), and E2E tests for critical paths only (slow but thorough). Result: Full test suite runs in under a minute.

**3. Maintainability**

By testing behavior (not implementation), our tests are resilient to refactoring. If you change how the Counter component stores state internally (say, from `$state` to a Svelte store), the integration tests still pass because the behavior didn't change.

### What We Sacrificed

**1. Simplicity**

Three test frameworks is more complex than one. You need to learn Vitest, Testing Library, and Playwright. You need to know when to use each. Trade-off: We accepted this complexity for better testing at each level.

**2. 100% Coverage**

We chose 90% coverage instead of 100%. Why? The last 10% is often not worth the effort:

- Boilerplate code (type definitions, config files)
- Error handling for impossible states
- Defensive code that "can't happen"

We can manually verify critical paths are at 100% (LocalStorage, random selection, box priority) while allowing some uncovered code elsewhere.

**3. E2E Test Coverage**

We don't E2E test every feature. E2E tests are expensive (slow, brittle, hard to debug). We use them for:

- Happy path user journeys (random selection → confirm → inventory updates)
- Critical workflows (rearrange → save)
- Browser/viewport-specific issues

We don't E2E test:

- Every edge case (that's unit testing's job)
- Every component in isolation (that's integration testing's job)

## Real-World Compromises

### Testing Library vs. Shallow Rendering

In the React world, there's a debate: shallow rendering (render component in isolation, mock children) vs. full rendering (render everything, including children).

**We chose full rendering.** Testing Library doesn't even support shallow rendering - you always render the real component tree. Why?

Because mocked children don't behave like real children. If your component passes props to a child, and the child breaks when receiving those props, shallow rendering won't catch it. Full rendering will.

The trade-off: Full rendering is slightly slower and tests can break when child components change. But we accepted this because it catches more real bugs.

### jsdom vs. Real Browser for Integration Tests

We configured Vitest to use **jsdom** (a JavaScript implementation of the DOM) instead of running tests in a real browser.

**Why jsdom?**

- Faster: No browser startup time
- Simpler: No browser automation needed
- Good enough: Handles most DOM interactions correctly

**When it falls short:**

- Complex CSS (flexbox edge cases, viewport units)
- Browser-specific APIs (Web Audio, WebRTC)
- Real scrolling behavior

For BroteinBuddy, jsdom is fine for integration tests. If we hit browser-specific issues, we'd write an E2E test to verify it in a real browser.

### Mobile-First E2E Testing

We test on **iPhone 13 Pro first**, then Desktop Chrome. In an ideal world, we'd test on:

- iPhone Safari (current iOS)
- iPhone Safari (previous iOS)
- Android Chrome (latest)
- Desktop Chrome
- Desktop Firefox
- Desktop Safari

But that's 6x the test time and 6x the maintenance burden. For a personal PWA, testing on iPhone (the target device) and Desktop Chrome (most common) is good enough.

If we were building for enterprise, we'd add more browsers. But we're not, so we didn't.

### Coverage Thresholds: 90% vs. 100%

We set the threshold at **90% overall**, with the expectation that critical paths are at **100%**.

**Why not 100% everywhere?** Diminishing returns. Consider this utility function:

```typescript
export function formatLocation(location: Location): string {
  if (!location) return 'Unknown'; // Edge case
  return `Stack ${location.x}, Height ${location.y}`;
}
```

To get 100% coverage, you need to test:

1. Valid location → "Stack 1, Height 2"
2. Null location → "Unknown"
3. Undefined location → "Unknown"

Test #1 is essential. Tests #2 and #3 are defensive - TypeScript prevents null/undefined with strict mode, so these should never happen in practice.

We'd write test #1 (90% coverage) and maybe test #2. Test #3 is overkill.

**For critical paths** (random selection algorithm, inventory mutations), we do aim for 100%. If the random selector breaks, users can't use the app. That deserves exhaustive testing.

## What You Should Learn From This

### 1. Pattern: Test at the Right Level

**Rule of thumb:**

- **Unit test:** Pure functions with no dependencies
- **Integration test:** Components with user interaction
- **E2E test:** Multi-step workflows across pages

Don't unit test UI. Don't E2E test algorithms. Match the test type to what you're verifying.

**Example of testing at the wrong level:**

```typescript
// BAD: E2E testing a pure function
test('capitalize function works', async ({ page }) => {
  await page.goto('/utils/capitalize?input=hello');
  expect(await page.textContent('.result')).toBe('Hello');
});

// GOOD: Unit test the function directly
it('capitalizes strings', () => {
  expect(capitalize('hello')).toBe('Hello');
});
```

### 2. Pattern: Test Behavior, Not Implementation

**Bad test:**

```typescript
it('has a count variable set to 0', () => {
  const wrapper = render(Counter);
  expect(wrapper.component.count).toBe(0); // Reaching into internals
});
```

**Good test:**

```typescript
it('displays count of 0 initially', () => {
  render(Counter);
  expect(screen.getByTestId('count').textContent).toBe('0'); // Testing what user sees
});
```

Why? The first test breaks if you rename `count` to `value`, even though the component still works. The second test only breaks if the user-visible behavior changes.

### 3. Pattern: Use Descriptive Test Names

Your test names should read like documentation:

```typescript
// BAD
it('works', () => {
  expect(capitalize('hello')).toBe('Hello');
});

// GOOD
it('capitalizes the first letter of a string', () => {
  expect(capitalize('hello')).toBe('Hello');
});
```

When this test fails, you want to instantly know what broke just from reading the name.

### 4. Pattern: Test Edge Cases and Error Paths

Happy path tests are easy. Edge cases are where bugs hide:

```typescript
describe('truncate', () => {
  it('truncates strings longer than maxLength', () => {
    expect(truncate('This is a long string', 10)).toBe('This is...');
  });

  it('handles empty strings', () => {
    expect(truncate('', 10)).toBe('');
  });

  it('handles strings exactly at maxLength', () => {
    expect(truncate('Exact', 5)).toBe('Exact');
  });

  it('handles edge case with very short maxLength', () => {
    expect(truncate('Hello', 3)).toBe('...');
  });
});
```

That last test? That's the one that catches the bug where `maxLength < 3` breaks your ellipsis logic.

### 5. Pattern: Arrange, Act, Assert (AAA)

Structure every test the same way:

```typescript
it('increments count when + button is clicked', async () => {
  // ARRANGE: Set up the test
  const user = userEvent.setup();
  render(Counter);

  // ACT: Perform the action
  const incrementButton = screen.getByTestId('increment');
  await user.click(incrementButton);

  // ASSERT: Verify the result
  const countElement = screen.getByTestId('count');
  expect(countElement.textContent).toBe('1');
});
```

This makes tests easy to read and understand.

## Common Pitfalls

### 1. Testing Implementation Details

**Symptom:** Tests break when you refactor, even though behavior didn't change.

**Example:**

```typescript
// BAD: Testing internal state
it('stores count in state variable', () => {
  const wrapper = render(Counter);
  expect(wrapper.component.count).toBe(0);
});
```

If you switch from `let count` to a Svelte store, this test breaks. But the component still works!

**Fix:** Test what the user sees, not how it's implemented.

### 2. Brittle Selectors

**Symptom:** E2E tests break when you change CSS classes or HTML structure.

**Example:**

```typescript
// BAD: Selector tied to implementation
await page.click('.btn.btn-primary.btn-lg.increment');
```

If you change the button's classes, this breaks.

**Fix:** Use test IDs or semantic selectors:

```typescript
// GOOD: Test ID
await page.click('[data-testid="increment"]');

// BETTER: Semantic selector
await page.click('button[aria-label="Increment count"]');
```

### 3. Over-Reliance on Coverage Metrics

**Symptom:** 100% coverage, but bugs still get through.

Coverage measures **lines executed**, not **scenarios tested**. You can have 100% coverage and still miss edge cases.

**Example:**

```typescript
export function divide(a: number, b: number): number {
  return a / b;
}

// This gives 100% coverage
it('divides numbers', () => {
  expect(divide(10, 2)).toBe(5);
});
```

But you never tested `divide(10, 0)`, which returns `Infinity`! Coverage doesn't catch missing tests for edge cases.

**Fix:** Use coverage as a safety net (catches untested code), but also think critically about edge cases.

### 4. Slow Test Suites

**Symptom:** Tests take 5+ minutes to run, so you stop running them.

**Causes:**

- Too many E2E tests (move to integration)
- Integration tests that could be unit tests
- Tests that wait for timeouts instead of specific conditions
- No parallelization

**Fix:**

- Move tests down the pyramid (E2E → integration → unit)
- Use `waitFor` instead of fixed delays
- Run tests in parallel
- Only E2E test critical paths

### 5. Flaky Tests

**Symptom:** Tests randomly fail and pass without code changes.

**Causes:**

- Race conditions (async code finishes in different order)
- Timeouts too short
- Tests depend on each other (test order matters)
- External dependencies (network, database)

**Fix:**

- Use `waitFor` or `waitForElementToBeRemoved` for async operations
- Make tests independent (each test sets up its own state)
- Mock external dependencies in unit/integration tests
- Increase timeouts for E2E tests in CI (slower environments)

## Going Deeper

### Recommended Reading

**Testing Library Philosophy:**

- [Testing Library Guiding Principles](https://testing-library.com/docs/guiding-principles/)
- [Common Mistakes with Testing Library](https://kentcdodds.com/blog/common-mistakes-with-react-testing-library)

**Testing Pyramid:**

- [The Practical Test Pyramid](https://martinfowler.com/articles/practical-test-pyramid.html) by Martin Fowler
- [Write Tests. Not Too Many. Mostly Integration.](https://kentcdodds.com/blog/write-tests) by Kent C. Dodds

**Vitest:**

- [Vitest Documentation](https://vitest.dev/)
- [Migrating from Jest to Vitest](https://vitest.dev/guide/migration.html)

**Playwright:**

- [Playwright Documentation](https://playwright.dev/)
- [Best Practices](https://playwright.dev/docs/best-practices)

### Related Concepts

**Test-Driven Development (TDD):**
Write tests before code. Red → Green → Refactor.

- Pros: Forces you to think about design, ensures testable code
- Cons: Can be slower initially, requires discipline
- When to use: Critical algorithms, complex business logic

**Behavior-Driven Development (BDD):**
Write tests in natural language (Given/When/Then).

- Tools: Cucumber, SpecFlow
- Best for: Non-technical stakeholders reviewing test cases
- We didn't use it: Overkill for solo project, but useful in teams

**Snapshot Testing:**
Save component output, fail if it changes.

- Pros: Fast way to detect unintended UI changes
- Cons: Easy to blindly update snapshots, doesn't test behavior
- When to use: Complex UI that shouldn't change, but sparingly

**Visual Regression Testing:**
Take screenshots, compare pixel-by-pixel.

- Tools: Percy, Chromatic, BackstopJS
- Best for: Catching visual bugs (misaligned buttons, color changes)
- We didn't use it: Overkill for v1, consider for later

## Questions to Think About

1. **When would you choose 100% coverage over 90%?**
   - Hint: Think about medical software, financial systems, or code that handles user data.

2. **How would you test a feature that depends on the current date/time?**
   - Hint: Dependency injection, mocking, or test fixtures?

3. **Should you test third-party library code?**
   - Hint: If you use Svelte's `$state`, should you test that it's reactive?

4. **When would you write an E2E test instead of an integration test?**
   - Hint: What can E2E tests catch that integration tests can't?

5. **How would you test a component that makes network requests?**
   - Hint: Unit test (mock fetch), integration test (MSW), or E2E test (real API)?

---

**Key Takeaway:** Testing isn't about hitting a coverage percentage or using the latest tools. It's about building confidence that your code works correctly, while keeping feedback fast and tests maintainable. Use the testing pyramid, test behavior over implementation, and choose the right test type for what you're verifying. Your future self (and users) will thank you.
