# Effective Code Review Documentation: Structure and Actionability

## What We Built

We created a structured format for code review documentation that makes reviews both comprehensive and actionable. This isn't just about finding problems - it's about organizing findings in a way that helps developers prioritize fixes, understand severity, and make informed decisions about technical debt.

The system uses a two-tier approach: a detailed review file saved in `.scratch/` for full analysis, and a concise summary posted as a PR comment. The detailed review examines code across 11 dimensions (from functional completeness to security), classifies findings by severity (Blocker/Important/Minor), and organizes enhancement ideas by impact and effort.

This format evolved from real-world pain points: reviews that were hard to scan, findings buried in prose, unclear priorities, and no way to distinguish "must fix now" from "nice to have someday." The structure we built solves these problems through consistent formatting, linked table of contents, and explicit severity classifications.

## Why This Approach

### The Problem We Solved

Code reviews are crucial for maintaining quality, but poorly structured reviews create problems:

**Unstructured reviews are hard to scan.** When findings are written as paragraphs of prose, you can't quickly answer: "What are the blockers?" or "How many important issues are there?" You have to read the entire review.

**Priorities get lost.** Without explicit severity classifications, everything feels equally important. A typo in a comment gets the same treatment as a security vulnerability.

**Enhancement ideas disappear.** Good reviews identify not just problems but opportunities for improvement. But if these are mixed with critical bugs, they get ignored or treated as blockers when they're really "nice to haves."

**Reviews don't guide action.** A review should answer: "What must I fix before merging? What should I fix immediately after? What can I defer to future PRs?" Without structure, developers are left guessing.

### Options We Considered

#### Option 1: Prose-Style Reviews
**The "write it like an essay" approach** - narrative reviews with findings embedded in paragraphs.

- Pros: Natural to write, flexible, easy to provide context
- Cons: Hard to scan, priorities unclear, easy to miss critical issues, no actionable checklist
- Why we didn't choose it: Developers need quick answers ("Can I merge?"), not essays to parse

#### Option 2: Inline PR Comments Only
**The "comment on the code" approach** - only leave comments on specific lines.

- Pros: Contextual (right at the problem), GitHub-native, easy to mark resolved
- Cons: No overview of all issues, can't see big picture, hard to track what's fixed, no severity classification, enhancement ideas get lost
- Why we didn't choose it: Great for pointing to specific problems, but terrible for seeing overall review status

#### Option 3: Structured Format with Severity Classifications ← **We chose this**
**The "table of contents + detailed sections" approach** - organize findings by type and severity with links.

- Pros: Scannable at a glance, clear priorities, separates problems from enhancements, actionable verdict, consistent format enables pattern recognition
- Cons: More upfront structure required, longer reviews (but only because we're thorough), requires discipline to maintain format
- Why we chose it: Balances completeness with scannability. Developers can quickly assess status and prioritize fixes, while still getting detailed analysis.

## How It Works

### The Two-Tier Structure

#### Tier 1: Detailed Review File

Saved to `.scratch/code-review-pr-{NUMBER}-{TIMESTAMP}.md`

This is the complete analysis. It includes:
- Full examination across all 11 dimensions
- Detailed findings with code references, technical analysis, and proposed solutions
- Enhancement ideas categorized by impact/effort matrix
- Links to related issues or documentation

**Why save to `.scratch/`?**
- Keeps reviews organized and searchable
- Doesn't clutter the repository (`.scratch/` is gitignored)
- Provides a paper trail of review history
- Can be referenced later when similar issues arise

#### Tier 2: PR Comment Summary

Posted to the PR as a comment.

This is the executive summary. It includes:
- Verdict (Approved / Fix X things before merge / etc.)
- Count of issues in each severity category
- Brief descriptions of blockers and important issues
- Link to detailed review file

**Why post a summary?**
- GitHub shows it directly in the PR
- Developers get immediate feedback without opening files
- Creates a permanent record in PR history
- Can be read on mobile or GitHub's web UI

### The Three-Part Format

Every detailed review follows this structure:

#### Part 1: Verdict and Summary

```markdown
# Code Review: PR #x - nth review

**Approved** / **Fix 2 things before merge** / **Fix 1 thing before merge and 2 things immediately after**

[Optional 8-128 word summary with compliments, context, or kudos]
```

**Purpose:** Immediate answer to "Can I merge this?"

The verdict should be unambiguous:
- **Approved**: Merge away, no blockers
- **Fix X things before merge**: You have X blockers, details below
- **Fix X before merge and Y immediately after**: X blockers + Y important issues that can't be fixed in this PR but must be addressed ASAP
- **Approved with immediate followup needed**: No blockers, but critical issues to address post-merge

The summary (8-128 words) is optional but valuable for:
- Acknowledging good work ("This PR shows excellent judgment...")
- Providing context ("The implementation handles the compatibility issue well...")
- Setting the tone (constructive, not adversarial)

**Why limit to 128 words?** Forces brevity. If you can't summarize the review in a few sentences, you're probably rambling.

#### Part 2: Findings Table of Contents

```markdown
## Findings

* [Problems](#problems)
  - [Blockers](#blockers): 3
    - [Logic error: Random library implemented incorrectly](#section_link)
    - [Test coverage: Missing async validation test](#section_link)
    - [Bug: Infinite scroll breaks shuffle feature](#section_link)
  - [Important](#important): 1
    - [Documentation: Architecture diagram contradiction](#section_link)
  - [Minor](#minor): 1
    - [Consistency: Test conventions](#section_link)
* [Enhancement ideas](#enhancement-ideas)
  1. [High impact, low effort](#high-impact-low-effort): 1
    - [Technical debt: Duplicated logic](#section_link)
  2. [High impact, moderate effort](#high-impact-moderate-effort): 0
  3. [Moderate impact, low effort](#moderate-impact-low-effort): 2
  4. [High impact, high effort](#high-impact-high-effort): 1
  5. [Moderate impact, moderate effort](#moderate-impact-moderate-effort): 0
  6. [Low impact, low effort](#low-impact-low-effort): 3
  7. [Moderate impact, high effort](#moderate-impact-high-effort): 0
  8. [Low impact, moderate effort](#low-impact-moderate-effort): 0
* [Nit](#nit)
```

**Purpose:** Scannable overview with counts and quick links.

This is where the format really shines. At a glance, you can answer:
- "How many blockers?" → Look at the count
- "What's blocking me?" → Read the linked titles
- "Are there any quick wins?" → Check "High impact, low effort"
- "What's the most important enhancement?" → Scan the impact/effort matrix

**Why numbered enhancement categories?** The impact/effort matrix creates 9 possible combinations (high/med/low impact × high/med/low effort). We organize them by priority:
1. High impact, low effort (do these ASAP)
2. High impact, moderate effort (plan for these)
3. Moderate impact, low effort (quick wins)
4. High impact, high effort (major initiatives)
5. Moderate impact, moderate effort (nice to haves)
6. Low impact, low effort (if you're bored)
7. Moderate impact, high effort (probably skip)
8. Low impact, moderate effort (probably skip)

You can omit categories with zero findings (show only what's relevant).

**Why link every finding?** Click the link, jump to the detailed section. No scrolling through the whole document looking for "that security thing."

#### Part 3: Detailed Findings

```markdown
## Problems

### Blockers

#### Blocker 1: Missing Playwright browser installation

**Location:** `DEVELOPING.md`, `package.json` scripts

**Issue:** The DEVELOPING.md documentation does not mention that Playwright
browsers must be installed before running E2E tests...

**Evidence:**
```
Error: browserType.launch: Executable doesn't exist at /Users/...
```

**Why this is a blocker:** New developers or CI/CD pipelines will immediately
encounter test failures...

**Proposed solution:**
1. Add to DEVELOPING.md under "Running Tests" section:
   ```markdown
   #### First-time Playwright Setup
   ...
   ```
2. Consider adding a `test:e2e:install` script...

---

### Important

#### Important 1: Integration test compatibility tracking

**Location:** `tests/integration/README.md`

**Issue:** The integration test directory contains only a README noting
compatibility issues...

**Why this is important:** Without tracking, this technical debt may be
forgotten...

**Proposed solution:**
1. Create a GitHub issue titled "..."
2. Add labels: `technical-debt`, `testing`, `dependencies`
...
```

**Purpose:** Comprehensive analysis with context and solutions.

Each finding should include:

**Location:** File path and line number (makes it easy to find)
- Good: `src/lib/storage.ts:45-52`
- Bad: "In the storage module"

**Issue:** What's wrong, stated clearly
- Be specific, not vague
- Include code snippets if helpful
- Explain the impact (what breaks, what's risky)

**Evidence (when applicable):** Error messages, test output, screenshots
- Especially important for bugs or test failures
- Proves the issue exists (not just theoretical)

**Why this is a [blocker/important/minor]:** Justify the severity
- Blockers: Prevents merge (missing requirements, security holes, crashes)
- Important: Fix ASAP after merge (too big for this PR but critical)
- Minor: Address later (maintainability, consistency, nice-to-haves)

**Proposed solution:** Don't just identify problems, solve them
- Offer 2-3 options if there are trade-offs
- Include code examples for fixes
- Link to documentation or similar patterns in the codebase

### The 11 Review Dimensions

Reviews examine code across these dimensions (in order of priority):

1. **Functional Completeness** - Does it meet all requirements?
2. **Code Quality** - Modern features, clear naming, conventions
3. **Architecture** - Patterns, separation of concerns, module boundaries
4. **Test Coverage** - Adequate tests, edge cases, error paths
5. **Documentation** - APIs documented, complex logic explained
6. **Edge Cases** - Null handling, empty collections, boundaries
7. **Error Handling** - Caught appropriately, clear messages, cleanup
8. **Performance** - Algorithm complexity, efficient queries, caching
9. **Maintainability** - Readable, minimal duplication, easy to modify
10. **Security** - Input validation, injection protection, no secrets
11. **Cross-Cutting Concerns** - Pattern consistency, polyglot interactions

**Why start with Functional Completeness?** If the code doesn't meet requirements, nothing else matters. Fix that first.

**Why 11 dimensions?** Comprehensive coverage without overlap. Each dimension focuses on a specific aspect of quality. Together, they catch most categories of issues.

You don't need to report on all 11 dimensions in every review. Only report findings - if there are no security issues, don't include an empty "Security" section.

### Severity Classifications

#### Blocker
**Must fix before merge.**

Examples:
- Missing acceptance criteria from plan/issue
- Unfixed blockers from previous review
- Security vulnerabilities (SQL injection, XSS, hardcoded secrets)
- Crashes or data corruption bugs
- Broken builds or failing tests
- Out-of-scope features that should be removed

**Test:** Would you feel comfortable merging this with this issue unfixed? If no → Blocker.

#### Important
**Fix immediately after merge.**

Examples:
- Issues too large to fix in this PR but critical (existing bugs you discovered)
- Technical debt that will cause problems soon
- Missing documentation for public APIs
- Performance issues that will bite users
- Inconsistencies with established patterns (creates confusion)

**Test:** Can you fix it in this PR without scope creep? If no, but it's critical → Important.

**Important means:** Create an issue, fix in next PR, ideally before moving to next feature.

#### Minor
**Address later.**

Examples:
- Maintainability improvements (minor refactoring)
- Minor inconsistencies (spacing, naming that's "okay but not great")
- Missing tests for unlikely edge cases
- TODOs or comments that should become issues

**Test:** Would anyone notice if this never got fixed? If no → Minor.

**Minor means:** Create an issue, prioritize with other backlog work, fix when convenient.

### Enhancement Categorization

Enhancements aren't problems - they're opportunities to improve code beyond the current requirements.

We categorize by **impact** (how much value it adds) and **effort** (how hard it is):

**Impact levels:**
- **High:** Significantly improves maintainability, performance, or user experience
- **Moderate:** Noticeable improvement but not transformative
- **Low:** Nice to have, marginal benefit

**Effort levels:**
- **Low:** < 1 hour (quick refactor, add a helper function)
- **Moderate:** 1-4 hours (small feature, modest refactor)
- **High:** > 4 hours (major refactor, infrastructure change, new system)

**Example enhancement:**

```markdown
### High impact, low effort

#### Enhancement 1: Extract duplicated validation logic

**Current situation:** The `addBox` and `updateBox` functions both validate
location conflicts with duplicated logic in `src/lib/stores.ts:45-60` and
`src/lib/stores.ts:112-127`.

**Impact:** High - Reduces duplication, single source of truth, easier to
maintain validation rules

**Effort:** Low - Extract to `validateLocation` helper, ~30 min

**Proposed implementation:**
```typescript
function validateLocation(location: Location, excludeBoxId?: string): ValidationResult {
  // Consolidated logic here
}
```

**Why this matters:** When we add new validation rules (e.g., max stack height),
we'll only need to change one place instead of two.
```

**Why categorize this way?** Helps prioritize technical debt. "High impact, low effort" items are slam dunks - do them. "Low impact, high effort" items are probably not worth it.

### When to Use Each Finding Type

**Problem (Blocker/Important/Minor) vs Enhancement:**

- **Problem:** Code doesn't meet current requirements or quality standards
  - Missing functionality from acceptance criteria → Blocker
  - Security vulnerability → Blocker
  - Untested error path → Important or Minor

- **Enhancement:** Code meets requirements, but could be better
  - Extract duplicated logic → Enhancement (high impact, low effort)
  - Add caching for performance → Enhancement (moderate impact, moderate effort)
  - Switch to better algorithm → Enhancement (high impact, high effort)

**Rule of thumb:** If it's in the acceptance criteria or a clear quality issue (security, bugs, no tests), it's a Problem. If it's "this would be nice" or "we could improve this," it's an Enhancement.

## Design Trade-offs

### What We Optimized For

**1. Scannability**

Developers shouldn't need to read 500 lines to know if they can merge. The table of contents with counts gives you the answer in 10 seconds.

We optimized for **pattern recognition** - every review has the same structure. Once you've read two reviews, you know exactly where to look for the information you need.

**2. Actionability**

Every finding should answer:
- Where is the problem? (file:line)
- What's wrong? (clear description)
- Why does it matter? (severity justification)
- How do I fix it? (proposed solutions)

No vague findings like "The error handling could be better." Instead: "Missing null check in `storage.ts:45` - if `localStorage.getItem()` returns null, `JSON.parse()` will throw. Add null check or use default value."

**3. Prioritization**

The severity classifications and impact/effort matrix make it clear what to fix now, soon, and eventually. This prevents "everything is important" syndrome.

**4. Consistency**

Same format for every review. Same severity levels. Same categories. This enables:
- Faster writing (template provides structure)
- Faster reading (you know the format)
- Pattern recognition (spot recurring issues across reviews)
- Tooling (could parse reviews programmatically)

### What We Sacrificed

**1. Brevity**

Structured reviews are longer than prose reviews. A simple "Looks good!" approval becomes a formatted document with sections.

**Trade-off we accepted:** Thoroughness over brevity. An extra minute to write a structured review saves 10 minutes for the developer trying to understand it.

**When we keep it brief:** For trivial changes (typo fixes, README updates), skip the full structure. Just comment "LGTM, approved."

**2. Flexibility**

The structure is prescriptive. You can't just write whatever you want - you need to classify findings by severity, organize enhancements by impact/effort, etc.

**Trade-off we accepted:** Structure over flexibility. The consistency is more valuable than the freedom to freestyle.

**When we break the rules:** For very unusual situations (e.g., reviewing a complete rewrite), adapt the format as needed. But default to the structure.

**3. Natural Flow**

Prose reviews can tell a story: "I noticed this issue, which led me to this concern, which made me think about that pattern..."

Structured reviews are more clinical: "Here are the blockers. Here are the important issues. Here are the enhancements."

**Trade-off we accepted:** Clarity over narrative. Save the storytelling for the optional summary section.

## Real-World Compromises

### When to Use the Full Structure

**Use the full structure for:**
- Feature implementations (Phase 1-3 deliverables)
- Infrastructure changes (CI/CD, testing, build tools)
- Bug fixes that touch multiple files
- Any PR where there are findings across multiple categories

**Skip the full structure for:**
- Trivial changes (typo fixes, comment updates)
- README or documentation-only PRs
- Automated dependency updates (if tests pass)

**How to decide:** If you're writing more than 3 findings, use the structure. If you have 1-2 small comments, just comment inline on the PR.

### When to Create Issues vs. Fix Immediately

**Blockers:** Must fix before merge. Don't create an issue - request changes on the PR.

**Important:** Create an issue immediately. Link it in the review. Assign it high priority. Fix before starting next feature.

**Minor:** Create an issue. Add to backlog. Prioritize with other work.

**Enhancements:** Create issues for high/moderate impact items. Skip issues for low-impact items unless you're tracking all technical debt.

**Why create issues?** Creates a paper trail, prevents items from being forgotten, allows prioritization, can be referenced in future PRs.

### Handling Disagreements

Sometimes the PR author disagrees with a finding's severity:
- Reviewer says "Blocker," author says "Minor"
- Reviewer says "High impact," author says "Low impact"

**How to resolve:**

1. **Discuss in the PR comments.** Explain your reasoning. Maybe you're wrong, maybe they are, maybe it's somewhere in between.

2. **Defer to project standards.** If CONTRIBUTING.md says "90% coverage required," and the PR has 85%, that's a blocker. No debate.

3. **Defer to the reviewer for security/correctness issues.** If the reviewer spots a SQL injection, that's a blocker even if the author thinks it's unlikely.

4. **Defer to the author for effort estimates.** If the reviewer says "low effort" but the author knows it requires refactoring half the codebase, trust the author.

5. **Use "Important" as a compromise.** If you can't agree whether it's a blocker, mark it Important - fix it post-merge.

### Balancing Thoroughness with Speed

Code reviews shouldn't block work for days. But they also shouldn't be rubber stamps.

**Guidelines:**

- **Aim for 24-hour turnaround** on PRs. Faster for small changes, slower for major features.
- **Use "Approved with followup" liberally.** If there are minor issues but the code is fundamentally sound, approve with tracked followup items.
- **Focus on high-value feedback.** Don't bikeshed (argue about trivial style issues) unless it violates documented standards.
- **Batch findings.** Review once thoroughly instead of multiple rounds of "oh, one more thing."

**When to do multiple rounds:**
- Blockers weren't fixed correctly (still broken)
- Fixes introduced new issues (verify the fix)
- Major refactor based on feedback (re-review the new approach)

**When to approve after one round:**
- All blockers fixed
- Important items tracked in issues
- Trust that minor items will be addressed

## What You Should Learn From This

### 1. Pattern: Structure Enables Speed

Ironically, adding structure makes reviews faster to write and read:

**Without structure:**
- Stare at blank page: "What should I say?"
- Write prose: "I noticed this... and also that... oh and another thing..."
- Reader struggles to extract action items

**With structure:**
- Fill in template: "What are the blockers? What's important? What's minor?"
- Organize findings under headings
- Reader scans table of contents, jumps to relevant sections

**When to use:** Any code review with multiple findings. The structure becomes muscle memory after 3-4 reviews.

**When to skip:** Single-issue reviews or trivial changes. "LGTM" is fine.

### 2. Pattern: Severity Based on Impact, Not Perfection

Don't block PRs on perfection. Block on:
- Missing requirements
- Broken functionality
- Security vulnerabilities
- Unfixed previous blockers

Everything else is a matter of priority:
- Will users/developers be impacted soon? → Important
- Will it matter eventually? → Minor
- Would it be nice but not necessary? → Enhancement

**Example:**

```typescript
// Finding: Missing error handling
function processData(data: unknown) {
  return JSON.parse(data)  // Throws on invalid JSON
}
```

**If this is critical path code** (user data, payment processing) → **Blocker** (will crash in production)

**If this is internal tool code** (dev script, test helper) → **Important** (should fix but won't break production)

**If this is example code** (documentation, tutorial) → **Minor** (nice to show best practices but not critical)

Same finding, different severity based on context.

### 3. Pattern: Propose Solutions, Don't Just Critique

**Bad finding:**
> "Error handling is insufficient."

**Good finding:**
> "Missing error handling in `storage.ts:45`. If `localStorage.getItem()` returns null, `JSON.parse()` will throw.
>
> Proposed fix:
> ```typescript
> const stored = localStorage.getItem('state')
> return stored ? JSON.parse(stored) : DEFAULT_STATE
> ```
>
> This safely handles missing or corrupted data."

**Why this matters:** The developer knows exactly what to do. No back-and-forth, no guesswork.

**When to propose multiple solutions:**
- Trade-offs exist (simple vs. robust, fast vs. correct)
- Different approaches with different implications
- You're not sure which is best

**Example:**

> "Proposed solutions:
> 1. Return default state (simple, but loses data on error)
> 2. Try/catch with fallback (robust, but hides errors)
> 3. Validate before parsing with Zod (most robust, adds dependency)
>
> Recommend option 2 for critical data, option 1 for non-critical."

### 4. Pattern: Use the Impact/Effort Matrix for Technical Debt

Not all improvements are worth doing. Categorizing by impact and effort helps prioritize:

**High impact, low effort:** Do these ASAP (quick wins)
- Extract duplicated logic
- Add missing type annotations
- Fix misleading variable names

**High impact, high effort:** Plan for these (major initiatives)
- Migrate to new framework version
- Refactor core architecture
- Add missing feature (complex)

**Low impact, low effort:** Do when bored (nice to haves)
- Fix typos in comments
- Improve test names
- Alphabetize imports

**Low impact, high effort:** Skip these (not worth it)
- Rewrite working code in "better" style
- Adopt trendy pattern with no real benefit
- Premature optimization

**Why this works:** Makes explicit that not all technical debt is equal. Helps teams prioritize limited time.

### 5. Pattern: Separate Detailed Analysis from Summary

**Detailed file** (`.scratch/review.md`):
- Complete analysis
- Code references
- Multiple proposed solutions
- Links to documentation
- Full context

**PR comment summary:**
- Verdict with counts
- Brief descriptions
- Link to detailed file

**Why separate?** Different audiences and purposes:
- Developer wants quick answer: "Can I merge?" → PR comment
- Developer fixing issues wants details: "How do I fix blocker 2?" → Detailed file
- Future developer investigating similar issue → Detailed file
- Historical record → Both (PR comment persists even if `.scratch/` file is lost)

**When to combine:** Very small PRs with 1-2 simple findings. Just comment inline on the PR, no need for separate file.

## Going Deeper

### Recommended Reading

**Code Review Best Practices:**
- [Google's Code Review Developer Guide](https://google.github.io/eng-practices/review/)
- [Conventional Comments](https://conventionalcomments.org/) - Structured comment format
- [How to Make Your Code Reviewer Fall in Love with You](https://mtlynch.io/code-review-love/) - Author's perspective

**Writing Technical Documentation:**
- [Divio Documentation System](https://documentation.divio.com/) - Four types of docs (tutorials, how-tos, reference, explanation)
- [Write the Docs](https://www.writethedocs.org/) - Community for technical writers

**Code Quality and Standards:**
- [The Checklist Manifesto](https://atulgawande.com/book/the-checklist-manifesto/) - Why checklists work
- [Continuous Code Reviews](https://medium.com/@9len/continuous-code-reviews-6e6e5e64acf8) - Making reviews part of workflow

### Related Concepts

**Conventional Comments:**
A similar structured approach to code review comments:

```
suggestion (non-blocking): Extract this to a helper function

issue (blocking): This will crash if data is null

praise: Excellent use of the builder pattern here
```

Labels like `praise`, `nitpick`, `suggestion`, `issue`, `question` clarify intent. Similar to our Blocker/Important/Minor classifications.

**Automated Code Review:**
Tools like SonarQube, CodeClimate, or ESLint can catch many code quality issues automatically:
- Style violations
- Common bugs (null checks, type errors)
- Security vulnerabilities
- Code duplication

Use these for the mechanical checks, save human reviewers for:
- Architecture and design decisions
- Business logic correctness
- Maintainability and readability
- Trade-offs and priorities

**Design Decision Records (ADRs):**
Similar structured format for documenting architectural decisions:
- Context: What's the situation?
- Decision: What did we choose?
- Consequences: What are the implications?

Same principle as structured reviews: consistent format enables faster understanding.

## Questions to Think About

1. **When would you classify a missing test as a Blocker vs. Important vs. Minor?**
   - Hint: Depends on whether it's a critical path, acceptance criteria, or nice-to-have.

2. **How would you handle a finding where the fix is expensive but the risk is low?**
   - Hint: Consider the impact/effort matrix and whether it's a Problem or Enhancement.

3. **Should code reviews check for things that automated tools can catch?**
   - Hint: What's the value of human time vs. machine time?

4. **How do you balance "approve fast to unblock development" vs. "thorough review to catch issues"?**
   - Hint: What can be fixed post-merge vs. pre-merge?

5. **When should you create a GitHub issue vs. just leaving a PR comment?**
   - Hint: What happens to PR comments after merge? What needs tracking?

---

**Key Takeaway:** Effective code reviews aren't about finding every possible issue - they're about identifying the important issues, communicating them clearly, and helping the team prioritize fixes. Structure enables this by making reviews scannable, findings actionable, and priorities explicit. The format is just a tool; the goal is better code and better collaboration.
